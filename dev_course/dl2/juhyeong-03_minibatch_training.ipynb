{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load autoreload extension and instnatiate with 2 secs\n",
    "# matplotlib inline\n",
    "\n",
    "# #export and import exp.nb_02 * and import torch nn functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load x,y trains and x,y valid\n",
    "# set n,m,c,nh as 50\n",
    "# define a model class with a layer for [lin,relu,lin]\n",
    "# get preds for x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]),\n",
       " torch.Size([50000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()\n",
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)            \n",
    "        return x                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define log_softmax(x)\n",
    "# log_softmax pred\n",
    "# define nll(inp, targ) negative log likelyhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return (x.exp() / x.exp().sum(dim=1, keepdim=True)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2314, -2.2209, -2.3749, -2.3563, -2.1197, -2.4826, -2.3490, -2.3665,\n",
       "         -2.3805, -2.1986],\n",
       "        [-2.3357, -2.2648, -2.2910, -2.3196, -2.1715, -2.4790, -2.3701, -2.3461,\n",
       "         -2.3947, -2.1064]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(pred)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define cross_entrophy\n",
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "But since our $x$s are 1-hot encoded, this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax(pred)\n",
    "def nll(inp, targ):\n",
    "    return - inp[range(targ.shape[0]), targ].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nll(sm_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3039, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the formula \n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax, which was previously defined as `(x.exp()/(x.exp().sum(-1,keepdim=True))).log()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of the $x_{j}$.\n",
    "\n",
    "In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify log_softmax(x)\n",
    "# test_near nll with new softmax against loss\n",
    "# implement logsumexp()\n",
    "#   m = x.max(-1)[0]\n",
    "# update log_softmax to use logsumexp()\n",
    "# replace with nll_loss, log_softmax\n",
    "\n",
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]    \n",
    "    return m + (x - m[:, None]).exp().sum(-1).log()\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - x.logsumexp(-1, keepdim=True)\n",
    "    \n",
    "test_near(loss, nll(log_softmax(pred), y_train))\n",
    "test_near(nll(log_softmax(pred), y_train), F.cross_entropy(pred, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy(out, yb)\n",
    "# set loss_func to cross_entrophy\n",
    "# training loop\n",
    "    # bx, by\n",
    "    # get loss\n",
    "    # backward\n",
    "    \n",
    "    # loop through model layers \n",
    "    # hasattr('weight') true, then update weights with lr\n",
    "    # make sure weight.grad.zero_()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    return (out.argmax(dim=-1) == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 1\n",
    "bs = 64\n",
    "loss_func = F.cross_entropy\n",
    "lr = 0.5\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        si = i * bs\n",
    "        ei = si + bs\n",
    "\n",
    "        xb = x_train[si:ei]\n",
    "        yb = y_train[si:ei]\n",
    "\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= lr * l.weight.grad\n",
    "                    l.bias -= lr * l.bias.grad\n",
    "\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train[:10]).log_softmax(dim=-1).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# redefine Model with F.relu with explicit function calling\n",
    "# print named_children()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def __call__(self, xb):\n",
    "        super.__call__()\n",
    "        return self.l2(F.relu(self.l1(xb)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 : Linear(in_features=784, out_features=50, bias=True)\n",
      "l2 : Linear(in_features=50, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = Model(m, nh, 10)\n",
    "\n",
    "for name, l in model.named_children():\n",
    "    print(f'{name} : {l}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range((n - 1) // bs + 1):            \n",
    "            sidx = i * bs\n",
    "            eidx = sidx + bs\n",
    "            \n",
    "            xb = x_train[sidx:eidx]\n",
    "            yb = y_train[sidx:eidx]\n",
    "            \n",
    "            pred = model(xb)            \n",
    "            loss = loss_func(pred, yb)\n",
    "            loss.backward()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= lr * p.grad                    \n",
    "                    p.grad.zero_()\n",
    "                                                        \n",
    "fit()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2172, grad_fn=<NllLossBackward>), tensor(0.8750))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0552, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                p -= p.grad * self.lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.data.zero_()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))\n",
    "opt = Optimizer(model.parameters())\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        si = i * bs\n",
    "        ei = si + bs\n",
    "\n",
    "        xb = x_train[si:ei]\n",
    "        yb = y_train[si:ei]\n",
    "\n",
    "        pred = model(xb)\n",
    "\n",
    "        loss = F.cross_entropy(pred, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1525, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0242, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 1. define Dataset() class with __init, __len, __getitem\n",
    "# train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "# train_ds[0:5]\n",
    "\n",
    "# get_model and run train loop\n",
    "# print loss and acc\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 784]), torch.Size([4]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "x, y = train_ds[0:4]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))\n",
    "opt = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epoch):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        si = i * bs\n",
    "        ei = si + bs\n",
    "        \n",
    "        xb, yb = train_ds[si:ei]\n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0211, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0211, grad_fn=<NllLossBackward>),\n",
       " <function __main__.accuracy(out, yb)>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs):\n",
    "        self.ds, self.bs = ds, bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs):\n",
    "            yield self.ds[i:i + self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16]), torch.Size([16, 784]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)\n",
    "# assert xb.shape==(bs,28*28)\n",
    "# assert yb.shape==(bs,)\n",
    "# next(iter(train_dl))\n",
    "yb.shape, xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([50000]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = train_ds[:]\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dl))\n",
    "def fit():\n",
    "    for epoch in range(n_epoch):\n",
    "        for xb, yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1318, grad_fn=<NllLossBackward>), tensor(0.9599))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc > 0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5, 7, 1, 2, 0, 9, 6, 3, 4, 8]),\n",
       " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small_ds = Dataset(*train_ds[:10])\n",
    "# s = Sampler(small_ds,3,False)\n",
    "# [o for o in s]\n",
    "# suffle, torch.randperm(n), torch.arange(n)\n",
    "\n",
    "\n",
    "torch.randperm(10), torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle):\n",
    "        self.ds, self.bs, self.shuffle = ds, bs, shuffle\n",
    "        self.m = len(ds)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(self.m) if self.shuffle else torch.arange(self.m)\n",
    "        for i in range(0, self.m, self.bs):\n",
    "            yield idxs[i:i + self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])],\n",
       " [tensor([6, 2, 5]), tensor([3, 9, 7]), tensor([0, 1, 8]), tensor([4])])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_ds = Dataset(*train_ds[:10])\n",
    "[el for el in Sampler(small_ds, 3, False)], [el for el in Sampler(small_ds, 3, True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x1a2def27c8>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(*ds[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.ds, self.bs, self.shuffle = ds, bs, shuffle\n",
    "        self.m = len(ds)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(self.m) if self.shuffle else torch.arange(self.m)\n",
    "        for i in range(0, self.m, self.bs): \n",
    "            yield idxs[i:i + self.bs]\n",
    "\n",
    "def collate(b):\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "            \n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn):\n",
    "        self.ds, self.sampler, self.collate_fn = ds, sampler, collate_fn\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for sidxs in self.sampler:            \n",
    "            yield self.collate_fn([self.ds[sidx] for sidx in sidxs])\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, Sampler(train_ds, bs, True), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a2dfbb080>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
